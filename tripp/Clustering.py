"""
    @release_date  : $release_date
    @version       : $release_version
    @author        : Christos Matsingos, Ka Fu Man 
    
    This file is part of the TrIPP software
    (https://github.com/fornililab/TrIPP).
    Copyright (c) 2024 Christos Matsingos, Ka Fu Man and Arianna Fornili.

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, version 3.

    This program is distributed in the hope that it will be useful, but
    WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU
    General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program. If not, see <http://www.gnu.org/licenses/>.
"""

import MDAnalysis as mda 
import pandas as pd 
from tripp.create_clustering_matrix import create_clustering_matrix 
from tripp.kmedoids_clustering_ import kmedoids_clustering_ 
from tripp.write_clustering_info import write_clustering_info 
from sklearn.metrics import silhouette_score 
import numpy as np 
from tripp.calculate_rmsd_matrix import calculate_rmsd_matrix 
from tripp.gromos_clustering_ import gromos_clustering_ 
from tripp.dbscan_clustering_ import dbscan_clustering_ 

class Clustering: 


    """ 
    This class provides a way to extract representative structures from a trajectory 
    using the clustering methods: kmedoids, gromos, or dbscan methods. Clustering 
    is done using the pKa values and (if required) the relative position of 
    selected residues as features. 
    
    The class takes as input: 

    trajectory_file: str, in the format allowed by MDAnalysis 

    topology_file: str, in the format allowed by MDAnalysis 

    pka_file: str, CSV file containing pka values generated by the Trajectory class 
    using the same trajectory_file and topology_file. 

    residues: list, list of residues for which the clustering will be done. The 
    residues have a pKa value assigned to them by PROPKA. 

    log_file: str, file name for log file 

    include_distances: bool, default=True, if True the relative positions 
    (as distances between the charge centers) are used as additional features 
    for the clustering alongside the pKa values. 
    """


    def __init__(self, trajectory_file, topology_file, pka_file, residues, log_file, include_distances=True):

        self.trajectory_file = trajectory_file
        self.topology_file = topology_file 
        self.pka_file = pka_file 
        self.residues = residues 
        self.log_file = log_file 
        self.include_distances = include_distances 
        self.universe = mda.Universe(topology_file, trajectory_file) 
        self.pka_df = pd.read_csv(self.pka_file, index_col='Time [ps]') 
        self.clustering_matrix, self.times, self.frames = create_clustering_matrix(self.universe, self.pka_df, self.residues, self.include_distances) 

    
    def kmedoids(self, automatic=False, n_clusters=8, metric='euclidean', method='alternate', init='hueristic', max_iter=300, random_state=None, max_clusters=20): 
        """
        This function implements the KMedoids method to do the clustering. 
        
        automatic: bool, default=False, if True the clustering is run 
        using various cluster numbers (defined by max_clusters). The 
        silhouette score is determined at each iteration and the 
        best number of clusters is chosen based on the highest 
        silhouette score. 
        
        n_clusters: int, default=8, number of clusters used for the 
        clustering. Ignored if automatic=True. 
        
        metric, method, init, max_iter, random_state as found in 
        sklearn_extra.cluster.KMedoids (https://scikit-learn-extra.readthedocs.io/en/stable/generated/sklearn_extra.cluster.KMedoids.html). 

        max_clusters: int, default=20, max number of clusters to use 
        when automatic=True. 
        """

        clustering_method = 'KMedoids' 
        
        if automatic == False: 
            labels, cluster_centers, medoid_indices = kmedoids_clustering_(n_clusters=n_clusters, metric=metric, method=method, init=init, max_iter=max_iter, random_state=random_state, clustering_matrix=self.clustering_matrix, frames=self.frames) 
            sil_score = silhouette_score(self.clustering_matrix, labels) 
            print(f'Clustering with {clusters} clusters produces an average silhouette score of {sil_score}.')
        
        elif automatic == True: 
            sil_scores = [] 
            cluster_nums = [] 
            for clusters in range(2,max_clusters+1): 
                labels, cluster_centers, medoid_indices = kmedoids_clustering_(n_clusters=clusters, metric=metric, method=method, init=init, max_iter=max_iter, random_state=random_state, clustering_matrix=self.clustering_matrix, frames=self.frames) 
                sil_score = silhouette_score(self.clustering_matrix, labels)
                sil_scores.append(sil_score) 
                cluster_nums.append(clusters) 
                print(f'Clustering with {clusters} clusters produces an average silhouette score of {sil_score}.')
            sil_scores = np.array(sil_scores) 
            best_cluster_n_index = np.argmax(sil_scores)
            sil_score = sil_scores[best_cluster_n_index] 
            n_clusters = cluster_nums[best_cluster_n_index] 
            labels, cluster_centers, medoid_indices = kmedoids_clustering_(n_clusters=n_clusters, metric=metric, method=method, init=init, max_iter=max_iter, random_state=random_state, clustering_matrix=self.clustering_matrix, frames=self.frames) 
            print(f'Best number of clusters identified at {n_clusters} with an average silhouette score of {sil_score}.') 

            #Write silhouette scores as additional CSV file. 
            df = pd.DataFrame({'Number of clusters' : cluster_nums, 'Average silhouette score' : sil_scores}) 
            df.to_csv(f'{self.log_file}_{clustering_method}_silhouette_scores.csv') 
        
        return write_clustering_info(self.universe, self.pka_file, self.times, self.frames, labels, cluster_centers, medoid_indices, self.log_file, clustering_method=clustering_method, sil_score=sil_score) 
    


    def gromos(self, automatic=False, cutoff=0.1, max_clusters=20, max_cutoffs=20): 

        """
        This function implements the gromos method to do the clustering 
        as described by Micheletti et al. 
        (DOI: 10.1002/1097-0134(20000901)40:4<662::aid-prot90>3.0.co;2-f). 
        
        automatic: bool, default=False, if True the clustering is run 
        using various cutoffs (number of cutoffs defined by max_cutoffs). 
        The silhouette score is determined at each iteration and 
        the best cutoff is chosen based on the highest silhouette 
        score. Cutoffs that produce more than the allowed max number 
        of clusters (defined by max_clusters) or that produce only one 
        cluster are excluded. 
        
        cutoff: float, default=0.1, cutoff used for clustering. Ignored 
        if automatic=True. 

        max_clusters: int, default=20, max number of clusters allowed 
        when automatic=True. 

        max_cutoffs: int, default=20, max number of cutoffs used to find 
        the most suitable one for clustering. 
        """
        clustering_method = 'GROMOS' 

        rmsd_matrix = calculate_rmsd_matrix(self.clustering_matrix, self.frames) 

        if automatic == False: 
            labels, cluster_centers, cluster_center_indices = gromos_clustering_(cutoff=cutoff, rmsd_matrix=rmsd_matrix, frames=self.frames) 
            sil_score = silhouette_score(self.clustering_matrix, labels) 
            print(f'Clustering with a cutoff of {cutoff} produces {len(set(labels))} clusters with an average silhouette score of {sil_score}.')
        
        elif automatic == True: 
            sil_scores = [] 
            max_rmsd = np.max(rmsd_matrix) 
            step = max_rmsd/max_cutoffs 
            cutoffs = [] 
            cluster_nums = [] 
            for cutoff_i in np.arange(step,max_rmsd+step,step): 
                labels, cluster_centers, cluster_center_indices = gromos_clustering_(cutoff=cutoff_i, rmsd_matrix=rmsd_matrix, frames=self.frames) 

                if len(labels) == len(set(labels)): 
                    print(f'Cutoff of {cutoff_i} produces no clusters. Moving on to next cutoff')  

                elif len(set(labels)) == 1: 
                    print(f'Cutoff of {cutoff_i} produces only one cluster. Moving on to next cutoff.') 

                elif len(set(labels)) > max_clusters: 
                    print(f'Cutoff of {cutoff_i} produces more than {max_clusters} clusters. Moving on to next cutoff') 

                else: 
                    cutoffs.append(cutoff_i) 
                    sil_score = silhouette_score(self.clustering_matrix, labels) 
                    sil_scores.append(sil_score) 
                    cluster_nums.append(len(set(labels))) 
                    print(f'Clustering with a cutoff of {cutoff_i} produces {len(set(labels))} clusters with an average silhouette score of {sil_score}.')

            sil_scores = np.array(sil_scores) 
            best_cutoff_index = np.argmax(sil_scores) 
            cutoff = cutoffs[best_cutoff_index] 
            sil_score = sil_scores[best_cutoff_index] 
            print(f'Best cutoff identified at {cutoff} with an average silhouette score of {sil_score}.') 
            labels, cluster_centers, cluster_center_indices = gromos_clustering_(cutoff=cutoff, rmsd_matrix=rmsd_matrix, frames=self.frames) 

            #Write silhouette scores as additional CSV file. 
            df = pd.DataFrame({'Number of clusters' : cluster_nums, 'RMSD cutoff' : cutoffs, 'Average silhouette score' : sil_scores}) 
            df.to_csv(f'{self.log_file}_{clustering_method}_silhouette_scores.csv') 
        
        return write_clustering_info(self.universe, self.pka_file, self.times, self.frames, labels, cluster_centers, cluster_center_indices, self.log_file, clustering_method=clustering_method, sil_score=sil_score) 
    

    
    def dbscan(self, automatic=False, max_clusters=20, eps=0.5, min_samples=5, metric='euclidean', metric_params=None, algorithm='auto', leaf_size=30, p=None, n_jobs=None): 
        
        """
        This function implements the DBSCAN method to do the clustering. 
        
        automatic: bool, default=False, if True the clustering is run 
        using various combinations of eps and min_samples values. The 
        silhouette score is determined at each iteration and 
        the best combination of values is chosen based on the highest 
        silhouette score. Combinations that produce more than the 
        allowed max number of clusters (defined by max_clusters) or 
        that produce only one cluster are excluded. 
        
        cutoff: float, default=0.1, cutoff used for clustering. Ignored 
        if automatic=True. 

        max_clusters: int, default=20, max number of clusters allowed 
        when automatic=True. 

        eps, min_samples, metric, metric_params, algorithm, 
        leaf_size, p, and n_jobs as found in 
        sklearn.cluster.DBSCAN (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html). 
        """

        clustering_method = 'DBSCAN' 

        if automatic == False: 
            labels, cluster_centers, cluster_center_indices = dbscan_clustering_(eps=eps, min_samples=min_samples, metric=metric, metric_params=metric_params, algorithm=algorithm, leaf_size=leaf_size, p=p, n_jobs=n_jobs, clustering_matrix=self.clustering_matrix, frames=self.frames, find_centroid=True) 
            sil_score = silhouette_score(self.clustering_matrix, labels) 
            print(f'Clustering with parameters eps={eps} and min_samples={min_samples} produces {len(set(labels))} clusters with an average silhouette score of {sil_score}.') 
            
        elif automatic == True: 
            sil_scores = [] 
            params = [] 
            cluster_nums = [] 
            for eps in np.arange(0.005, 0.505, 0.005): 
                for min_samples in np.arange(2, 11, 1): 
                    labels = dbscan_clustering_(eps=eps, min_samples=min_samples, metric=metric, metric_params=metric_params, algorithm=algorithm, leaf_size=leaf_size, p=p, n_jobs=n_jobs, clustering_matrix=self.clustering_matrix, frames=self.frames, find_centroid=False)

                    if len(set(labels)) == len(labels): 
                        print(f'Clustering with parameters eps={eps} and min_samples={min_samples} produces no clusters. Moving on to next set of parameters') 

                    elif len(set(labels)) <= 2: 
                        print(f'Clustering with parameters eps={eps} and min_samples={min_samples} produces only one cluster. Moving on to next set of parameters') 

                    elif len(set(labels)) > max_clusters: 
                        print(f'Clustering with parameters eps={eps} and min_samples={min_samples} produces more than {max_clusters} clusters. Moving on to next set of parameters') 

                    else: 
                        sil_score = silhouette_score(self.clustering_matrix, labels) 
                        sil_scores.append(sil_score) 
                        cluster_nums.append(len(set(labels))) 
                        params.append([eps, min_samples]) 
                        print(f'Clustering with parameters eps={eps} and min_samples={min_samples} produces {len(set(labels))} clusters with an average silhouette score of {sil_score}.') 
                        
            sil_scores = np.array(sil_scores) 
            best_params_index = np.argmax(sil_scores) 
            best_eps, best_min_samples = tuple(params[best_params_index]) 
            sil_score = sil_scores[best_params_index] 
            print(f'Best clustering parameters identified at eps={best_eps} and min_samples={best_min_samples} with an average silhouette score of {sil_score}.') 
            labels, cluster_centers, cluster_center_indices = dbscan_clustering_(eps=best_eps, min_samples=best_min_samples, metric=metric, metric_params=metric_params, algorithm=algorithm, leaf_size=leaf_size, p=p, n_jobs=n_jobs, clustering_matrix=self.clustering_matrix, frames=self.frames, find_centroid=True)
            params = np.array(params) 

            #Write silhouette scores as additional CSV file. 
            df = pd.DataFrame({'Number of clusters' : cluster_nums, 'Epsilon' : params[:,0], 'Minimum samples' : params[:,1], 'Average silhouette score' : sil_scores}) 
            df.to_csv(f'{self.log_file}_{clustering_method}_silhouette_scores.csv') 
            
        return write_clustering_info(self.universe, self.pka_file, self.times, self.frames, labels, cluster_centers, cluster_center_indices, self.log_file, clustering_method=clustering_method, sil_score=sil_score) 
    
